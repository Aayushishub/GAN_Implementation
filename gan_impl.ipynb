{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677c3600-419e-48d7-aa18-9eb23b9918d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('Cardiovascular_Disease_Dataset.csv')  # Replace with your dataset path\n",
    "features = df.iloc[:, :-1].values  # Features (13 columns)\n",
    "labels = df.iloc[:, -1].values  # Binary target (0 or 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)  # Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac5bda0d-36cd-47f2-bf6b-c91b1c90403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.values.astype('float32')\n",
    "features = (features - features.min(axis=0)) / (features.max(axis=0) - features.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69e0f89-a65a-40a8-b7b6-377db8474fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Generator Loss: 0.04555933177471161, Discriminator Loss: 4.068548679351807\n",
      "Epoch 100/1000, Generator Loss: -2.5829949378967285, Discriminator Loss: 1.062143325805664\n",
      "Epoch 200/1000, Generator Loss: 0.32400456070899963, Discriminator Loss: -0.4675522446632385\n",
      "Epoch 300/1000, Generator Loss: 1.5044524669647217, Discriminator Loss: -1.2937288284301758\n",
      "Epoch 400/1000, Generator Loss: -0.07329723238945007, Discriminator Loss: 0.41540175676345825\n",
      "Epoch 500/1000, Generator Loss: -1.205414891242981, Discriminator Loss: 1.416489839553833\n",
      "Epoch 600/1000, Generator Loss: 0.7858185768127441, Discriminator Loss: -0.8817535638809204\n",
      "Epoch 700/1000, Generator Loss: -0.0331575870513916, Discriminator Loss: 0.022462081164121628\n",
      "Epoch 800/1000, Generator Loss: -0.45740824937820435, Discriminator Loss: 0.9616893529891968\n",
      "Epoch 900/1000, Generator Loss: 0.0797375813126564, Discriminator Loss: -0.4087076187133789\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "noise_dim = 16\n",
    "epochs = 1000\n",
    "lambda_gp = 10  # Gradient penalty coefficient\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Build the generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_dim=noise_dim),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(features.shape[1])  # Output matches feature dimensions\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build the discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_dim=features.shape[1]),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Output a single scalar for WGAN\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Gradient penalty function\n",
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    batch_size = tf.shape(real_data)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0, dtype=tf.float32)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated, training=True)\n",
    "    grads = tape.gradient(pred, interpolated)\n",
    "    grads_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    return tf.reduce_mean((grads_norm - 1.0) ** 2)\n",
    "\n",
    "# Instantiate models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "def weight_init(shape, dtype=None):\n",
    "    return tf.random.normal(shape, stddev=0.02, dtype=dtype)\n",
    "\n",
    "# Apply to each layer\n",
    "generator.layers[0].kernel_initializer = weight_init\n",
    "discriminator.layers[0].kernel_initializer = weight_init\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    batch_size = tf.shape(real_data)[0]\n",
    "    noise = tf.random.normal([batch_size, noise_dim], dtype=tf.float32)\n",
    "\n",
    "    # Train Discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_data = generator(noise, training=True)\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(fake_data, training=True)\n",
    "        gp = gradient_penalty(discriminator, real_data, fake_data)\n",
    "        disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + lambda_gp * gp\n",
    "\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    # Train Generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_data = generator(noise, training=True)\n",
    "        fake_output = discriminator(fake_data, training=True)\n",
    "        gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            real_data = dataset[i:i + batch_size]\n",
    "            real_data = tf.convert_to_tensor(real_data, dtype=tf.float32)  # Ensure float32\n",
    "            gen_loss, disc_loss = train_step(real_data)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n",
    "\n",
    "# Train the WGAN\n",
    "train(features, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cef48f3-398a-48c6-a261-85b5b92d6a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Synthetic Data:\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0  -0.186791  -0.072328   1.711799   0.496327   0.444162   1.003199   \n",
      "1  -0.430517  -0.000606   1.159562   0.437986   0.519269   0.941822   \n",
      "2  -0.182776   0.210462   1.656037   0.539154   1.120157   0.418823   \n",
      "3   0.250664   0.174054   1.715939   0.075578   0.463148   0.827110   \n",
      "4   0.099859  -0.278009   1.285033   0.384079   0.571976   0.883264   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  feature_11  feature_12  \\\n",
      "0   1.161993   0.516793   1.087988    0.784765    1.100292    0.031853   \n",
      "1   0.666007   0.139558   0.291172    0.343342    0.708575    0.586641   \n",
      "2   0.757976   0.130427   1.206694    0.256304    0.438453    1.233352   \n",
      "3   0.690844   0.345309   1.356646    0.547505    0.637118   -0.150519   \n",
      "4   0.319432   0.162772   0.984755    0.833282    1.415848    0.812654   \n",
      "\n",
      "   feature_13  feature_14  \n",
      "0    0.468930    0.714676  \n",
      "1    0.653042    0.919934  \n",
      "2    0.821630    1.370047  \n",
      "3    0.329990    0.826678  \n",
      "4    0.711170    0.793336  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming the 'generator' model is already trained\n",
    "# 'noise_dim' should match the input size of your generator model\n",
    "\n",
    "def generate_synthetic_data(generator, num_samples, noise_dim):\n",
    "    # Generate random noise input for the generator\n",
    "    noise = tf.random.normal([num_samples, noise_dim], dtype=tf.float32)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    synthetic_data = generator(noise, training=False).numpy()\n",
    "    \n",
    "    # Convert synthetic data to a DataFrame\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=[f'feature_{i+1}' for i in range(synthetic_data.shape[1])])\n",
    "    \n",
    "    return synthetic_df\n",
    "\n",
    "# Example: Generate 500 synthetic samples\n",
    "synthetic_data_df = generate_synthetic_data(generator, num_samples=500, noise_dim=noise_dim)\n",
    "\n",
    "# Display the generated synthetic data\n",
    "print(\"Generated Synthetic Data:\")\n",
    "print(synthetic_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "456206ef-d8b6-43d4-9e2a-105975a926c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>chestpain</th>\n",
       "      <th>restingBP</th>\n",
       "      <th>serumcholestrol</th>\n",
       "      <th>fastingbloodsugar</th>\n",
       "      <th>restingrelectro</th>\n",
       "      <th>maxheartrate</th>\n",
       "      <th>exerciseangia</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>noofmajorvessels</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103368</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119250</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119372</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>133</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132514</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>146211</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patientid  age  gender  chestpain  restingBP  serumcholestrol  \\\n",
       "0     103368   53       1          2        171                0   \n",
       "1     119250   40       1          0         94              229   \n",
       "2     119372   49       1          2        133              142   \n",
       "3     132514   43       1          0        138              295   \n",
       "4     146211   31       1          1        199                0   \n",
       "\n",
       "   fastingbloodsugar  restingrelectro  maxheartrate  exerciseangia  oldpeak  \\\n",
       "0                  0                1           147              0      5.3   \n",
       "1                  0                1           115              0      3.7   \n",
       "2                  0                0           202              1      5.0   \n",
       "3                  1                1           153              0      3.2   \n",
       "4                  0                2           136              0      5.3   \n",
       "\n",
       "   slope  noofmajorvessels  target  \n",
       "0      3                 3       1  \n",
       "1      1                 1       0  \n",
       "2      1                 0       0  \n",
       "3      2                 2       1  \n",
       "4      3                 2       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d8b7d-bfc0-41f8-bdc8-15ddfbba1f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
